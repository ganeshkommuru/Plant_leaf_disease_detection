{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import Packages\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import mahotas\n",
    "import cv2\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters\n",
    "'''\n",
    "images_per_class       = 10\n",
    "fixed_size             = tuple((500, 500))\n",
    "train_path             = \"C:\\\\Users\\\\ganesh appu\\\\Downloads\\\\project_imageprocessing\\\\Plant_leave_diseases_dataset_with_augmentation\\\\\"\n",
    "h5_train_data          = 'C:\\\\Users\\\\ganesh appu\\\\Downloads\\\\project_imageprocessing\\\\train_data.h5'\n",
    "h5_train_labels        = 'C:\\\\Users\\\\ganesh appu\\\\Downloads\\\\project_imageprocessing\\\\train_labels.h5'\n",
    "bins                   = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function for convertion of Image format from BGR TO RGB\n",
    "'''\n",
    "def rgb_bgr(image):\n",
    "    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return rgb_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converting the Image from RGB to HSV color space\n",
    "'''\n",
    "def bgr_hsv(rgb_img):\n",
    "    hsv_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
    "    return hsv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Image Segmentation\n",
    "'''\n",
    "def img_segmentation(rgb_img,hsv_img):\n",
    "    lower_green = np.array([25,0,20])\n",
    "    upper_green = np.array([100,255,255])\n",
    "    healthy_mask = cv2.inRange(hsv_img, lower_green, upper_green)\n",
    "    result = cv2.bitwise_and(rgb_img,rgb_img, mask=healthy_mask)\n",
    "    lower_brown = np.array([10,0,10])\n",
    "    upper_brown = np.array([30,255,255])\n",
    "    disease_mask = cv2.inRange(hsv_img, lower_brown, upper_brown)\n",
    "    disease_result = cv2.bitwise_and(rgb_img, rgb_img, mask=disease_mask)\n",
    "    final_mask = healthy_mask + disease_mask\n",
    "    final_result = cv2.bitwise_and(rgb_img, rgb_img, mask=final_mask)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global Feature: 1 Shape\n",
    "Hu moments\n",
    "'''\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global Feature: 2 Texture\n",
    "Haralick Texture\n",
    "'''\n",
    "def fd_haralick(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    return haralick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Global Feature: 3 Color\n",
    "'''\n",
    "def fd_histogram(image, mask=None):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Background_without_leaves', 'Blueberry___healthy', 'Cherry___Powdery_mildew', 'Cherry___healthy', 'Corn___Cercospora_leaf_spot Gray_leaf_spot', 'Corn___Common_rust', 'Corn___Northern_Leaf_Blight', 'Corn___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
     ]
    }
   ],
   "source": [
    "train_labels = os.listdir(train_path)\n",
    "train_labels.sort()\n",
    "print(train_labels)\n",
    "global_features = []\n",
    "labels          = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ganesh appu\\\\Downloads\\\\project_imageprocessing'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed folder: Apple___Apple_scab\n",
      "processed folder: Apple___Black_rot\n",
      "processed folder: Apple___Cedar_apple_rust\n",
      "processed folder: Apple___healthy\n",
      "processed folder: Background_without_leaves\n",
      "processed folder: Blueberry___healthy\n",
      "processed folder: Cherry___Powdery_mildew\n",
      "processed folder: Cherry___healthy\n",
      "processed folder: Corn___Cercospora_leaf_spot Gray_leaf_spot\n",
      "processed folder: Corn___Common_rust\n",
      "processed folder: Corn___Northern_Leaf_Blight\n",
      "processed folder: Corn___healthy\n",
      "processed folder: Grape___Black_rot\n",
      "processed folder: Grape___Esca_(Black_Measles)\n",
      "processed folder: Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "processed folder: Grape___healthy\n",
      "processed folder: Orange___Haunglongbing_(Citrus_greening)\n",
      "processed folder: Peach___Bacterial_spot\n",
      "processed folder: Peach___healthy\n",
      "processed folder: Pepper,_bell___Bacterial_spot\n",
      "processed folder: Pepper,_bell___healthy\n",
      "processed folder: Potato___Early_blight\n",
      "processed folder: Potato___Late_blight\n",
      "processed folder: Potato___healthy\n",
      "processed folder: Raspberry___healthy\n",
      "processed folder: Soybean___healthy\n",
      "processed folder: Squash___Powdery_mildew\n",
      "processed folder: Strawberry___Leaf_scorch\n",
      "processed folder: Strawberry___healthy\n",
      "processed folder: Tomato___Bacterial_spot\n",
      "processed folder: Tomato___Early_blight\n",
      "processed folder: Tomato___Late_blight\n",
      "processed folder: Tomato___Leaf_Mold\n",
      "processed folder: Tomato___Septoria_leaf_spot\n",
      "processed folder: Tomato___Spider_mites Two-spotted_spider_mite\n",
      "processed folder: Tomato___Target_Spot\n",
      "processed folder: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "processed folder: Tomato___Tomato_mosaic_virus\n",
      "processed folder: Tomato___healthy\n",
      "completed Global Feature Extraction...\n"
     ]
    }
   ],
   "source": [
    "for training_name in train_labels:\n",
    "    dir = os.path.join(train_path, training_name)\n",
    "\n",
    "    current_label = training_name\n",
    "    #Images in each file\n",
    "    for x in range(1,images_per_class+1):\n",
    "        file = dir + \"\\\\\" + \"image (\" +str(x) + \")\" +\".jpg\"\n",
    "\n",
    "        # Reading the image and converting it to required size\n",
    "        image = cv2.imread(file)\n",
    "        image = cv2.resize(image, fixed_size)\n",
    "\n",
    "        \n",
    "        # Calling Functions\n",
    "        \n",
    "        RGB_BGR       = rgb_bgr(image)\n",
    "        BGR_HSV       = bgr_hsv(RGB_BGR)\n",
    "        IMG_SEGMENT   = img_segmentation(RGB_BGR,BGR_HSV)\n",
    "\n",
    "        # Extracting Features\n",
    "        \n",
    "        fv_hu_moments = fd_hu_moments(IMG_SEGMENT)\n",
    "        fv_haralick   = fd_haralick(IMG_SEGMENT)\n",
    "        fv_histogram  = fd_histogram(IMG_SEGMENT)\n",
    "        \n",
    "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "        \n",
    "        #Storing the features\n",
    "        labels.append(current_label)\n",
    "        global_features.append(global_feature)\n",
    "\n",
    "    print(\"processed folder: {}\".format(current_label))\n",
    "\n",
    "print(\"completed Global Feature Extraction...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Features are (390, 532)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Overall vector size used for training the model is\n",
    "'''\n",
    "print(\"No of Features are {}\".format(np.array(global_features).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels (390,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Overall Training Label size\n",
    "'''\n",
    "print(\"Training Labels {}\".format(np.array(labels).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels encoded\n"
     ]
    }
   ],
   "source": [
    "# Encoding the Training Labels to Decrease the size\n",
    "targetNames = np.unique(labels)\n",
    "le          = LabelEncoder()\n",
    "target      = le.fit_transform(labels)\n",
    "print(\"Training labels encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector normalized\n"
     ]
    }
   ],
   "source": [
    "#Normalizing the feature vectors for size reduction\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler            = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features = scaler.fit_transform(global_features)\n",
    "print(\"Feature vector normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target labels: [ 0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  2  2  2  2\n",
      "  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4\n",
      "  4  4  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9\n",
      "  9  9  9  9 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11\n",
      " 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 14 14 14 14\n",
      " 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16\n",
      " 16 16 17 17 17 17 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19\n",
      " 19 19 19 19 19 19 19 19 20 20 20 20 20 20 20 20 20 20 21 21 21 21 21 21\n",
      " 21 21 21 21 22 22 22 22 22 22 22 22 22 22 23 23 23 23 23 23 23 23 23 23\n",
      " 24 24 24 24 24 24 24 24 24 24 25 25 25 25 25 25 25 25 25 25 26 26 26 26\n",
      " 26 26 26 26 26 26 27 27 27 27 27 27 27 27 27 27 28 28 28 28 28 28 28 28\n",
      " 28 28 29 29 29 29 29 29 29 29 29 29 30 30 30 30 30 30 30 30 30 30 31 31\n",
      " 31 31 31 31 31 31 31 31 32 32 32 32 32 32 32 32 32 32 33 33 33 33 33 33\n",
      " 33 33 33 33 34 34 34 34 34 34 34 34 34 34 35 35 35 35 35 35 35 35 35 35\n",
      " 36 36 36 36 36 36 36 36 36 36 37 37 37 37 37 37 37 37 37 37 38 38 38 38\n",
      " 38 38 38 38 38 38]\n",
      "Target labels shape: (390,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Target labels: {}\".format(target))\n",
    "print(\"Target labels shape: {}\".format(target.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset#1\": shape (390, 532), type \"<f8\">"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To save the feature vector on the local system\n",
    "h5f_data = h5py.File(h5_train_data, 'w')\n",
    "h5f_data.create_dataset('dataset#1', data=np.array(rescaled_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset#1\": shape (390,), type \"<i8\">"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To save target labels\n",
    "h5f_label = h5py.File(h5_train_labels, 'w')\n",
    "h5f_label.create_dataset('dataset#1', data=np.array(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f_data.close()\n",
    "h5f_label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import warnings\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable-parameters\n",
    "num_trees = 100\n",
    "test_size = 0.20\n",
    "seed      = 9\n",
    "train_path = \"C:\\\\Users\\\\ganesh appu\\\\Downloads\\\\project_imageprocessing\\\\Plant_leave_diseases_dataset_with_augmentation\\\\\"\n",
    "h5_train_data    = 'C:\\\\Users\\\\ganesh appu\\\\Downloads\\\\project_imageprocessing\\\\train_data.h5'\n",
    "h5_train_labels  = 'C:\\\\Users\\\\ganesh appu\\\\Downloads\\\\project_imageprocessing\\\\train_labels.h5'\n",
    "scoring    = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training labels\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "# sort the training labels\n",
    "train_labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(random_state=seed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] features shape: (390, 532)\n",
      "[STATUS] labels shape: (390,)\n",
      "[STATUS] training started...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names   = []\n",
    "\n",
    "h5f_data  = h5py.File(h5_train_data, 'r')\n",
    "h5f_label = h5py.File(h5_train_labels, 'r')\n",
    "\n",
    "\n",
    "global_features_string = h5f_data['dataset#1']\n",
    "global_labels_string   = h5f_label['dataset#1']\n",
    "\n",
    "global_features = np.array(global_features_string)\n",
    "global_labels   = np.array(global_labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "# verify the shape of the feature vector and labels\n",
    "print(\"[STATUS] features shape: {}\".format(global_features.shape))\n",
    "print(\"[STATUS] labels shape: {}\".format(global_labels.shape))\n",
    "\n",
    "print(\"[STATUS] training started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] splitted train and test data...\n",
      "Train data  : (312, 532)\n",
      "Test data   : (78, 532)\n"
     ]
    }
   ],
   "source": [
    "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n",
    "                                                                                          np.array(global_labels),\n",
    "                                                                                          test_size=test_size,\n",
    "                                                                                          random_state=seed)\n",
    "\n",
    "print(\"[STATUS] splitted train and test data...\")\n",
    "print(\"Train data  : {}\".format(trainDataGlobal.shape))\n",
    "print(\"Test data   : {}\".format(testDataGlobal.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.45611734e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        9.75225694e-01, 8.80787807e-02, 5.24793828e-01],\n",
       "       [9.86357503e-01, 3.60436573e-02, 4.91621560e-04, ...,\n",
       "        9.75225376e-01, 8.80702946e-02, 5.24794375e-01],\n",
       "       [9.94721836e-01, 1.85143470e-03, 1.23738979e-04, ...,\n",
       "        9.75224418e-01, 8.73303449e-02, 5.24782975e-01],\n",
       "       ...,\n",
       "       [9.43665634e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        9.75174616e-01, 9.00897562e-02, 5.24953176e-01],\n",
       "       [9.95389681e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        9.75243392e-01, 9.26997184e-02, 5.24845759e-01],\n",
       "       [9.76347475e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        9.75220452e-01, 8.82675311e-02, 5.24755592e-01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataGlobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.330444 (0.103874)\n",
      "LDA: 0.166734 (0.053750)\n",
      "KNN: 0.288710 (0.056436)\n",
      "CART: 0.288105 (0.082209)\n",
      "RF: 0.480847 (0.062254)\n",
      "NB: 0.246673 (0.065582)\n",
      "SVM: 0.112399 (0.050791)\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf  = RandomForestClassifier(n_estimators=num_trees, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=9, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(trainDataGlobal, trainLabelsGlobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=clf.predict(testDataGlobal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 10, 15, 28, 29, 20, 36, 36, 14,  5, 38,  1, 27, 29,  1,  6, 31,\n",
       "       17,  1, 18, 28, 34, 28,  1, 14,  8, 14, 13, 15, 27, 12,  2, 35,  9,\n",
       "       24,  2, 38,  3, 24, 15, 11, 11, 29, 23,  9, 13, 29, 13, 27, 15, 37,\n",
       "        1, 25, 38, 23,  3, 14, 10, 35, 23,  7,  7, 29, 29,  2, 15, 13, 32,\n",
       "       27, 21, 29, 15,  4,  9, 24, 12,  3,  5], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5512820512820513"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testLabelsGlobal, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
